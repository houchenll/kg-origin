## 1 知识表示简介

### 1.1 什么是知识表示
知识表示是指将知识以某种形式表示出来，以便计算机能够理解和处理。它是将现实世界中的信息、概念、实体、关系等转化为计算机可以处理的数据结构或符号形式的过程。知识表示的目的是为了让计算机能够模拟人类的认知和推理能力，从而解决各种复杂的任务。知识表示可以包括符号表示和分布式表示两种主要方式。

在符号表示中，知识以符号、规则或语言的形式进行描述，如逻辑表示法、产生式规则表示法、框架表示法、语义网络表示法等。这种表示方法注重抽象和逻辑推理，能够表达复杂的关系和规则，但可能在处理大规模数据时效率较低。

而分布式表示则是将知识转换为向量或矩阵等低维连续空间中的数值表示。通过使用机器学习和深度学习等方法，将实体、关系等转化为向量表示，以便计算机可以通过计算向量之间的相似度来推断和处理知识。这种表示方法具有高效处理大规模数据和学习隐含语义的优势，如知识表示学习中的嵌入算法、张量分解模型、神经网络模型等。

知识表示在人工智能领域中扮演着重要的角色，它是使计算机能够理解和运用知识的关键一步，为各种智能应用和系统提供了基础。通过合适的知识表示方法，计算机可以处理和推理复杂的知识，实现类似人类的认知和智能行为。

### 1.2 知识表示的发展
知识表示的发展历史可以追溯到人工智能的早期阶段，以下是知识表示在不同时期的主要里程碑和发展趋势：

早期符号表示方法：1950s - 1960s，知识表示主要采用基于符号逻辑的方法。这些方法包括逻辑表示法、产生式规则表示法等。逻辑表示法，其中最著名的是一阶逻辑（First-Order Logic，FOL），使用谓词逻辑和一阶逻辑等形式来表示知识和推理规则，而产生式规则表示法使用规则集合来描述知识和推理过程。这些方法注重逻辑推理和符号处理，但在处理大规模数据和不确定性方面存在局限性。

框架表示法和语义网络：1970s - 1980s，框架表示法将知识组织为一种结构化的形式，语义网络表示法则采用图形结构，将知识以节点和边的形式表示，节点表示实体，边表示实体之间的关系。

RDF和知识图谱：1990s - 2000s，随着万维网的兴起，知识表示的重点逐渐转向语义网。语义网是一个基于万维网的知识图谱，其中的知识以三元组（实体-关系-实体）的形式表示。为了标准化语义网的数据表示，万维网联盟（W3C）制定了资源描述框架（RDF）作为一种通用的数据格式。RDF以三元组为基础，将知识表示为主语、谓词和宾语的形式，为知识的联通性和共享提供了一种标准化的方式。

分布式表示和知识表示学习：近年来，随着深度学习的快速发展，分布式表示成为知识表示的新趋势。分布式表示将实体和关系映射为低维连续向量，在向量空间中表示知识，其中的距离和相似度可以用于推断和计算。知识表示学习是一种通过机器学习和深度学习方法，从大规模知识库中学习实体和关系的向量表示的技术。这些方法包括张量分解模型、神经网络模型、翻译模型、距离模型、双线性模型等，通过学习低维稠密的向量表示，提高了处理大规模知识和计算语义相似度的效率。

总体而言，知识表示的发展历史经历了从早期的符号表示到语义网和RDF的转变，再到如今的分布式表示和知识表示学习。随着技术的进步和理论的发展，知识表示越来越注重处理大规模和复杂的知识，以及学习隐含语义和推理能力。这些进展为构建更智能的系统和应用提供了基础，推动了人工智能的发展。

## 2 符号表示技术

### 2.1 逻辑表示法
逻辑表示法依据逻辑的复杂程度有命题逻辑、一阶谓词逻辑、高阶谓词逻辑。命题逻辑定义了具有真假值的原子命题，而复合命题可以通过与、或、非等逻辑联结词由原子命题连接而成，根据逻辑联结词真值表可以进行简单的推理功能。一阶谓词逻辑是在命题逻辑的基础上引入了存在量词(比如“有一个”、“至少有一个” “任何一个”等)和全称量词(比如“所有”、“每一个”、“全部”等)用来量化实体和概念。与一阶谓词逻辑不同的是，高阶谓词逻辑不仅可以量化实体和概念， 还可以量化谓词或集合。

### 2.2 框架表示法
20 世纪 70 年代，美国人工智能专家 Minsky 根据人类对知识的记忆方式提出了使用框架来表示知识的方法，人类在对现实世界的事物认知的过程中，通常会采用类似框架的结构将事物存储在记忆当中。例如，公司的人事部门会采用某种框架对求职者的个人信息进行记忆和存储，其中的求职者是一类人，槽(slot)是求职者的属性，人事部门需要做的就是对这些槽进行填充。框架对现实世界中的事物进行了抽象，用来表示事物的属性和事物之间的类属关系。对于一个新的实例，只需将实例的属性填入相对应的槽即可。框架具有结构化、继承性等优点，但是构建复杂的框架需要较高的成本，对知识的表达不够灵活。

框架表示法在表示结构性知识方面具有较好的效果，框架表示法形象的把现实事物都存储在框架中，当新的实体添加进来时，只需要在框架集合中找到合适的框架，并对框架模型基于实际情况进行适配，修改后的框架能够对知识进行合理的表示。某个具体的知识可以在框架集合中的特定框架得到表示，而不同的框架形成了对现实世界不同事物的表示，因此，框架链接在一起形成了框架系统，协调变化地表示不同地知识。

### 2.3 语义网络表示法
早在 19 世纪 60 年代，语义网络表示法(Semantic Network)由著名认知 学专家奎林(Quillan M Ross)提出。语义网络是一种有向图，它将知识表示成节点和边相互关联的模式，图中的节点代表实体、概念、事件、属性等，边代表实体之间、实体与概念之间、实体与属性之间等语义关系。语义网络用一种简单而统一的方式描述知识，方便计算机存储和检索知识，但是语义网络中的节点和边的值没有统一的标准，完全由用户自行定义，不能区分概念节点和对象节点，不利于计算机进行推理。

### 2.3.1 RDF/RDFS/OWL
随着研究的深入，对知识表示的要求也越来越高，早期的语义网络等方法便无法满足需求。W3C 推出了 资源描述框架(Resource Description Framework，RDF) 用来作为知识表示框架。RDF 实际上就是一个数据模型，它制定了一个描述知识的标准，使用 RDF 表示的知识，既可以让人读得懂，也可以让机器进行处理。

RDF 中的知识以三元组的形式表示，知识图谱中的知识都能够被划分成(Subject，predicate，object)， 例如(中国，首都，北京)。RDF 的顶点和边是具有较好的描述性，与此同时，多个互相关联的 RDF 还能够进行关系的组合拼接。RDF 序列化的方式有 RDF/XML，RDFa，JSON-LD 等。

由于 RDF 无法区分类和实体以及表达能力不足等缺点，因此提出 RDFS 和 OWL 这两种本体描述语言。RDFS/OWL 和 RDF 在序列化方式上基本相同，只不过 RDFS/OWL 包含了预定义词汇的集合，相当于是 RDF 的扩展。

RDFS(RDF Schema)是以 RDF 为基础，提供了属性和概念的基本类型系统，在基础性语义表达上具有良好的应用。

OWL Web 本体语言(OWL Web Ontology Language)是在 2002 年由 W3C 提出的，其目的是更好地表达复杂场景下类、属性等特征，开发更加复杂地语 义网络，它是在 RDF 基础上进行地拓展。OWL 的三个子语言:OWL Lite、OWL DL、OWL Full。区别在于 OWL Lite 适用于单个分类层次和简单属性约束的用户，OWL DL 在 OWL Lite 的基础上拓展了逻辑可判定约束，OWL Full 可以在 预定义词汇表上添加词汇。

#### 2.3.1.1 SPARQL 查询语言
与 RDF 一样，SPARQL 也是 W3C 推荐的一款技术，它是为 RDF 开发的一种查询语言和数据获取协议。

## 3 知识图谱嵌入技术（KGE）
虽然 RDF 等符号化表示方法可以直观地定义知识数据并易于扩展更新，但是无法进行直接的数值化计算，从而无法作为知识推理、智能问答等下游机器学习任务的输入数据。

在自然语言处理领域中，一般将词用向量表示出来，主要方法分为独热表示和分布式表示。其中，独热表示向量的维度等于词表大小，只有当前词对应的维度值可以为 1。 独热表示的显著缺点在于任意词向量之间都是孤立的，无法表示出语义的关联信息。分布式表示的理论基础来源于分布假说，即语义相似的词具有相似的上下文。

为了解决知识图谱符号化表示中存在的数据稀疏性以及无法进行数值计算的问题，研究人员将分布式表示引入到知识图谱领域，使用低维、稠密、连续的向量表示实体和关系信息，使学习到的向量能够捕获网络的结构信息、语义信息和属性信息，以支撑下游机器学习算法的数值计算。其中嵌入向量的维度d是固定的，一般在 [50, 1000]的区间内。

最开始对于网络的研究是从网络的拓扑结构出发的，忽略网络中节点和关系的类型信息，即将网络视为同构网络以矩阵分解的方式利用邻接矩阵实现节点嵌入。然而，真实网络中包含的节点类型是多样化 的，节点之间也存在着不同含义的作用关系，这种复杂的内容单纯依靠传统网络学习方法是无法提取的。另外，这种信息多样化的异构信息网络中还包含描述节点自身特征的属性信息，如文本、 图像等，因此需要有效的方式对这些信息进行融合学习。

这种统计学上的分布式表示能够捕获实体间的拓扑结构和语义信息，尤其是隐式关系，方便使用余弦相似度、欧氏距离等方法计算相似度。同时也缓解了独热编码造成的数据稀疏问题，提高了计算效率。还有一个优点是可以将异质信息投影到统一的语义空间，融合多知识库的信息，也便于知识库进行迁移。

知识表示学习主要有基于翻译的模型、语义匹配模型、结构表示方法、距离模型、融合多源信息的模型、神经网络模型、基于二阶近邻的模型、基于随机游走的模型等。

### 3.1 神经网络模型
其主要思想为，通过神经网络模型对知识图谱的结构特征进行学习以完成对知识图谱的建模，最后通过得到的参数模型对实体和关系进行低维度的向量化表示。神经网络模型包括了 SME、NTN和CapsE等。

#### 3.1.1 SME 模型
SME 模型是一种典型的基于线性神经网络的方法。在将实体和关系嵌入到同一向量空间后，SME 模型通过二分支神经网络分别对头部实体与关系、以及尾部实体 与关系进行融合，最后将两个融合结果进行内积后作为评分函数的值。

#### 3.1.2 NTN 模型
NTN 模型是一种基于非线性神经网络的方法。NTN 模型通过引入非线性层来提高对三元组的学习能力。NTN 定义了一连串与关系的嵌入向量相关的参数，并将标准线性神经网络层替换为双线性张量层，并且将经过非线性激活函数后的双线性 张量层的输出，与关系的嵌入向量在线性输出层进行交互。

#### 3.1.3 MLP
Multi-Layer Perceptron 方法是比NTN更要简单的一种基于神经网络的方法。给定一个事件(h,r,t)，MLP先将实体和关系的向量h,r,t拼接起来作为神网络的输入层，然后经过一层非线性隐藏层，最后接入一个线性输出层。

#### 3.1.4 NAM
Neural Association Model 方法使用深度神经网络架构进行建模。给定一个事件（h,r,t），NAM方法首先将头实体向量和关系向量拼接起来作为输入层，然后接入一个由L层非线性隐藏层组成的深度神经网络，最后将第L层的结果与尾实体向量一同输入到线性输出层。

#### 3.1.5 ConvE
ConvE 方法则使用卷积神网络架构进行建模。给定一个事实（h,r,t），ConvE方法首先将头实体向量和关系向量进行二维重塑，然后接入一个卷积层，再接入一个全连接层，最后将其结果与尾实体向量一同输入到线性输出层。

#### 3.1.6 CapsE 模型
CapsE 模型一种基于复杂神经网络的方法。CapsE 模型使用了胶囊神经网络对三 元组中的实体和关系之间的语义信息进行建模。首先，CapsE 模型通过将头部实体 嵌入向量 、关系嵌入向量和尾部实体嵌入向量 进行拼接，得到一个三列的矩阵。其次，CapsE 模型将矩阵输入到卷积层，在卷积层中操作多个过滤 器以得出不同的特征图。这些特征图被重建为相应的胶囊，然后将其路由到另一个胶囊以生成连续向量。最后，CapsE 模型将生成的向量的长度作为三元组的评分。

### 3.2 语义匹配模型 (Semantic Matching)
通过设计基于相似度的目标函数，在低维向量空间匹配不同实体和关系类型的潜在语义，定义基于相似性的评分函数，度量一个关系三元组的合理性。该类模型认为训练集中存在的关系三元组应该有较高的相似度，而训练集中没有的关系应该有相对较低的相似度。

又叫做双线性模型或张量分解方法（Tensor Decomposition, TD），其主要思想为，首先将头和尾部实体表示为低维度向量，其次将关系表示为矩阵，最后将头和尾部实体嵌入向量与关系矩阵进行数学运算。

此类模型包括了线性语义匹配的 TATEC、Rescal、DisMult、ComplEx等，和神经网络的语义匹配 SME、NTN等。

#### 3.2.1 Rescal 模型
Rescal（REgularized Singular value decomposition for Collaborative filtering）模型是一种常用的知识图谱表示学习模型，主要用于关系预测和知识图谱补全任务。该模型由Maximilian Nickel等人于2011年提出。

Rescal模型基于矩阵分解的思想，将知识图谱中的实体和关系表示为低维向量空间中的向量。具体来说，Rescal模型将每个实体表示为一个实体向量，每个关系表示为一个关系矩阵。实体向量和关系矩阵的乘积可以用于预测实体之间的关系。

在Rescal模型中，每个实体向量和关系矩阵都被限制为低秩的，这样可以降低模型的复杂性，并且在表示学习过程中捕捉到实体和关系之间的潜在语义信息。模型通过最小化预测值与真实关系之间的误差，学习到实体和关系的向量表示。

Rescal模型的优点是能够处理多对多的关系和多重关系。它能够推断知识图谱中缺失的关系，并进行关系预测。此外，Rescal模型还引入了正则化项，以防止过拟合，并提高模型的泛化能力。

Rescal模型在知识图谱中的应用包括实体关系预测、链接预测、知识图谱补全等任务。通过学习到的实体和关系的向量表示，可以进行关系推理、路径推理和知识图谱的语义搜索等操作，从而丰富知识图谱的应用和挖掘。

相对于现在普遍使用的基于翻译的知识表示方法，更注重挖掘向量化后实体与关系之间的潜在语义，更精准地刻画两者之间的复杂关系，同时模型的泛化能力强，便于后续推荐的使用。

#### 3.2.2 DistMult 模型
为了简化计算的复杂度，DistMult 将 Mr 限制为对角矩阵，不仅降低了算法复杂度，而且比其他算法在性能上有明显的提升。

#### 3.2.3 Complex 模型
在原始的 DistMult 模型中，每个关系所对应的头、尾实体均是对称的。所以 ComplEx 模型使用复数值嵌入的方式将 DistMult 模型中的对称关系扩展为非对称关系。

### 3.3 基于随机游走 (Random Walk-based models,RW)
基于随机游走的表示方法使用截断随机游走采样，生成节点序列，并将其类比为自然语句输入到 skip-gram 模型，最后输出节点的分布式表示向量，代表模型有 DeepWalk， node2ve 和 MetaPath2Vec。其核心思想在于通过最大化节点出现概率满足样本经验分布来学习节点的表示向量，并希望关系越紧密的节点对应的向量在空间中距离越近。

随机游走方法适用于学习知识图谱中节点的结构特征。

基于随机游走的方法能有效捕获远距离节点的相似度信息，同时也能将节点属性信息和局部结构特征融入嵌入过程中，提高信息保留量。由于随机游走带来的感受野非常大，并且能非常灵活地融入各种辅助信息，因此是当前的研究热点。

模型的编码器一般是直接嵌入，解码器则是一个 softmax 函数。模型的求解目标则是使解码的二元节点邻近度近似于经验邻近度。

多数RW类模型都可视为DeepWalk模型在异 质网络中的扩展。

#### 3.3.1 DeepWalk
DeepWalk 是最早引入自然语言处理中的词向量思想的网络节点表示方法，使用类似于深度优先搜索的随机游走来拓展节点的邻居。

Deep Walk 利用节点向量来表示节点与节点之间的关系，随机游走的方式在图中进行节点采样。给定当前访问的起始节点，从与之相邻的节点中随机采样作为下一个访问节点;重复此过程，达到访问序列长度满足的预设条件，得到足够序列后进行向量学习，从而反映图的结构特征。因此，Deep Walk 包含随机游走采样节点序列和学习表达向量两个步骤。
1. 将项目知识图谱作为同构网络，从网络中的每个实体开始进行 Random Walk 采样，得到局部相关联的结构化序列数据。
2. 对采样数据进行 Skip Gram 训练，或者当作文本中的单词输入至 word2vec 模型，将知识图谱中的节点向量化表示，最大化节点共现。两个项目共有的邻近节点越多，则对应表示向量的相似性越高，距离越短。

#### 3.3.2 node2vec
node2vec 是基于 DeepWalk 的改进模型，为了通过序列学习网络结构的结构相似性和内容相似性，提出了一种带偏置的随机游走策略，在随机游走过程中引入了广度优先搜索(BFS)和深度优先搜索(DFS)的策略。其中，内容相似性是指网络中紧密相连的节点应用具有相近的表示相连；结构相似性是指网络中具有相似作用的节点，它们未必直接相连，例如各局部范围的中心节点。总之，node2vec 通过最大化序列中的节点出现的概率来保持节点之间的高阶相似性。

#### 3.3.3 MetaPath2Vec
以上方法主要准对同质图网络，即网络中所有节点的类型只有 1 种。对于知识图谱这种异质网络(Heterogeneous Network)，不同类型的节点具有不同的特性，若按照相同的规则进行随机游走会丢失网络的结构特征和语义信息。因此，Dong 等人提出 MetaPath2Vec模型，使用元路径控制随机游走只在特定的类型之间进行游走。元路径是通过一组关系连接多个节点类型的路径，可以用来描述异质网络中不同类型对象之间不同的语义关系。基于元路径的表示学习的核心思想是使用元路径指导随机游走重构节点的异质邻居，并用异质的 skip-gram 模型学习节点的分布式表示。

#### 3.3.4 HINE
HINE 模型首先将异质文献 网络分解为多个子网络，然后在每个子网络中分别采 样随机游走序列，并分别使用 skip-gram 模型进行建 模，最后模型将损失函数求和统一进行优化。

### 3.4 基于二阶近邻
基于二阶近邻的表示方法为了解决网络中一阶直连关系的稀疏性问题，引入了二阶近邻关系作为补充，代表模型有：LINE、SDNE。二阶近邻关系描述的是节点邻域的相似性，即二阶相似性：两个实体的共同邻居越多代表它们的语义关系越相似。

LINE 模型输入是由节点独热编码构成的样本三元组，然后经过嵌入层网络，计算每个节点和上下文节点的一阶相似度或二阶相似度，然后通过损失函数进行优化，最后得到节点的嵌入表示向量。

SDNE 模型可以看作是 LINE 模型的扩展，其相似度定义与 LINE 相同，主要区别 在于 SDNE 使用自动编码器(autoencoder)结构来同时优化一阶相似度和二阶相似度。 因此，SDNE 模型输出的节点的分布式表示向量能够保留局部和全局结构信息，并且对稀疏性网络图具有鲁棒性。

### 3.5 神经张量模型
神经张量模型可以将实体之间的复杂关系表述出来，通过重复利用单词向量并取平均值构建实体，牺牲了复杂度，训练时所需的数据量较大。当知识图谱规模较大并且数据不完整时很难达到理想结果。

### 3.6 基于距离的方法
在最早的几个知识表示方法中结构表示方法(structured embedding，SE)有着代表性的地位，大多的表示算法都是在此基础之上进行了改进。在 SE 算法中，所有实体都被映射到同一个𝑑维度的空间向量中进行表示。

SE 方法是通过在给定关系𝑟的两个矩阵上对三元组中头实体向量𝑙h和尾实体向量𝑙𝑡在关系𝑟上进行距离计算。通过向量距离来表示头实体和尾实体在关系 r 上的关联性。其中头实体、尾实体向量和两个关系矩阵𝑀𝑟,1、 𝑀𝑟,2都是 SE 模型中的重要参数。通过优化模型参数使得损失函数的值降低从而更 真实的反映出实体和关系之间的语义信息。

单层神经网络模型 (singleayermodel，SLM)为了解决SE算法中对实体和关系联系描述不准确的问题，SLM提出使用单层神经网络的非线性操作的解决方法。SLM 模型在 SE 的基础上改进效果不大，但是在计算复杂度上却远超 SE，得 不偿失。

### 3.7 基于矩阵分解的模型 （Matrix Factorization-based models,MF）
基于矩阵分解的模型通常采用直接编码函数，即其编码函数是一个节点嵌入矩阵 Z ∈ Rd × |V| 与指示每个节点编码的独热向量v的乘积。模型的解码函数则通常定义为两个节点嵌入的内积。

在这些模型中，模型分解的邻近度矩阵都是网络的低阶邻接矩阵，其生成的特征表示仅能捕获网络中的低阶结构特征，然而网络中的高阶结构特征也非常重要。

总体上，基于矩阵分解的模型的编码和解码函数相对比较简单。但是，这类模型一般不对网络中的附属属性信息进行编码。同时，一些考虑高阶结构邻近度的 MF 模型要分解的矩阵通常是一个稠密矩阵，这对于大规模网络来说，存储和处理数据需要消耗大量的系统资源，在普通计算平台上难以实施。

#### 3.7.1 PTE 模型
PTE模型就是一个典型的 MF 类 模型。该模型首先将语料库中的文本共现信息和部 分单词标记信息表示成包含三个子网络的异质网络; 然后，模型针对每个子网使用 LINE 模型来建模;最 后，再将三个子网络联合起来统一训练求解节点的特 征表示。不过，PTE 模型在联合多个子网训练时，给不同子网赋予了相同的权重，这可能导致偏斜性问题。

#### 3.7.2 MVE 模型
MengQu 等人提出了一个 MVE 模型，该模型通过引入注意力机制，较好地解决了子网权重的自动求解问题。

### 3.8 基于自编码器的模型
与其他几类模型不同，本类模型的输入不再是一个表示节点编码的独热向量，而是基于网络结构定义的节点邻接向量 s  i，该向量中包含了异质网络中节点的结构信息。模型则使用一个自编码器结构将该邻接向量压缩成低维向量，并作为节点的特征表示。这类模型的一般思路也是 首先对异质网络中单个子网的节点邻接向量信息进 行压缩，然后再将多个子网的节点特征聚合起来。

AE 类模型的整体优势在于能够方便地使用多种现有的自编码器框架( 例 如 M L P ，V A E ，S A E 等 等 )对节点向量进行压缩。但是，针对大规模超大规模的网 络，AE类模型的输入维度很大，模型中的训练参数非常多，训练的复杂度过高。同时，这类模型通常是直 推式的，难以处理动态变化的网络。

该类型模型有：SHINE、DHNE、AMVAE等。

#### 3.8.1 SHINE
SHINE 模型针对在线异质社交网络中的情感链接 分类问题，首先使用三个自编码器框架对不同子网中 的用户节点邻接向量进行压缩;然后再将三个子网生 成的节点特征表示聚合起来得到最后的特征表示。 该模型仅能够捕获网络的二阶结构特征。

## 4 基于翻译的模型
又叫做距离模型(Distance Model，DM)，或平移模型(Translational Model，TM)。

该类模型将知识图谱中的每个关系看作从主体向量到客体向量的一个平移变换。通过最小化平移转化的误差，将知识图谱中的实体和关系类型映射到低维空间。核心思想是尽可能的保证 `h` 和 `r` 的向量与 `t` 向量接近，即保证 `h+r≈t`。

采用的是基于距离的评分函数，用头实体通过关系进行翻译以后的实体和尾实体之间的距离来测量事实的合理性。

基于翻译的方法只考虑了相邻实体间的连接关系，但实体可能与有路径相连的多跳邻居实体之间也有关系，这种关系基于翻译的方法没有考虑，因此本模型适用于实体和关系的预处理。

基于翻译的模型有：`TransE`、`TransH`、`TransR`、`Trans`、`TransA`、`TranSparse`、`TransG`等。

### 4.1 TransE
TransE 模型受 word2vec 中词向量平移不变的启发，对于给定的三元组实例(h，r，t)，把关系向量 r 看作是从头实体向量 h 到尾实体向量 t 之间的翻译，训练中不断对三元组进行调整，使得 (h+r) 尽可能等于 t，最终使得即 ||h + 𝑟|| ≈ 𝑡，得分函数为：`𝑓(h,𝑟,𝑡)=‖𝒉+𝒓−𝒕‖2`，是 L1 或者 L2 范数。一个三元组的得分函数值越接近 0，这个三元组越有可能为正确的三元组。

在训练过程中，TransE 算法随机地将每个三元组中的头、尾实体替换为其他候选实体，以生成不符合事实的错误三元组集合 S' ，并作为负样例参与模型参数的优化。

TransE 模型参数少，计算简单，在处理一对一的关系时性能较好。缺点是只能处理一对一关系，在遇到自反，一对多，多对一，多对多等复杂关系时，不同实体在同一关系中会有相同的向量表示，继而导致模型性能降低。 例如在知识图谱中包含（流浪地球，演员，吴京）以及（战狼，演员，吴京）两个三元组，TransE 模型会将“流浪地球”以及“战狼”之间紧密联系，其实它们是不同实体。

#### 4.1.1 TransE-SNS
传统的 TransE 模型在进行负采样的时，一般采用的随机替换方法，等概率地从实体中抽取一个实体替换尾实体或头实体，会产生许多错误的负例，在一定程度上降低了模型训练结果的效果。本算法使用 K- means 算法对实体进行聚类，使得被替换的实体和替换的实体同属于一个簇。

### 4.2 TransH
为了解决 TransE 模型只能处理一对一关系的问题，TransH 为每个关系引入了超平面的概念来替代原有的关系向量，使得同一个实体在不同的关系超平面中的表示不完全相同，解决了一对多和多对一的关系表达问题。

TransH 使用特定关系的平移向量 dr 和关系超平面的法向量 wr 来表示某一特定的关系r，对于每个事实三元组（h, r, t），将头实体 h 以及尾实体 t 沿着法向量 wr 映射到同一向量空间的关系超平面上，h⊥ 和 t⊥ 表示投影向量，然后h⊥ 与 t⊥ 再由关系的平移向量 dr 连接起来，在超平面上满足 ||h⊥ + dr||≈t⊥。其余部分和 TransE 相同。

### 4.3 TransR
TransE 以及 TransH 都是以知识图谱中的实体与关系在同一语义空间为前提，但一个实体可能包含有许多不同的语义，在不同关系下的含义可能不同，如同一个人在家庭关系中和在工作关系中的含义会有不同，而不同的关系也可能集中在实体的不同方面。因此，即使两实体在同一语义空间距离相近，但并不代表在另外的关系空间中实体表征仍然相似。

TransR 模型提出不同的关系关注实体不同属性，应具有不同的语义空间，将实体映射到不同的关系语义空间中进行翻译。TransR 模型通过变换矩阵 𝑀𝑟，将头尾实体的表征向量映射到相应的关系空间中后，再建立头尾实体的翻译关系，其得分函数为：𝑓(h，t)=‖hr + 𝑟 − 𝑡r ‖2，其中 hr = 𝒉𝑴𝑟，tr = 𝒕𝑴𝑟，其中 𝑴𝑟 表示实体空间投影到关系 𝑟 空间的投影矩阵。

与 TransE 和 TransH 相比，TransR 在性能上有了很大的进步。然而，它也有一些不足：1、对于关系 r ，头尾实体共用同一个投影矩阵 Mr，而Mr只和关系有关 ，但可以直观地看出头尾实体之间的类型或属性可能有本质上的不同。例如，在事实三元组(Elon Musk, Founder of, SpaceX)中，“Elon Musk”表示一个人，而“SpaceX”则表示 一家公司，它们是两种不同类型的实体。2、在某一实体从实体空间映射到特定关 系空间时，若将投影矩阵设置为仅与关系相关的矩阵时，这种实体与关系之间的 交互很可能无法获取完整的信息。3、由于使用投影矩阵，增加了参数，TransR 需要大量的计算资源，使得模型训练的复杂度变高。

#### 4.3.1 CTransR
Lin 等认为仅为每个关系学习唯一的关系向量还不足以建立从头实体到尾实体的所有语义信息转移，对于同一个关系𝑟来讲，𝑟具有多种语义上的表示。因此，在 TransR 模型的基础上又提出了 CTransR 模型，对于每个特定的关系，该模型首先通过 TransE 模型学习所有三元组(h, 𝑟, 𝑡)的表示向量𝐡, 𝐫, 𝐭，然后根据𝐡 − 𝐭对所有的实体对进行聚类，为每个聚类学习单独的关系向量𝐫𝑐 ,对每个关系学习关系矩阵 𝐌𝑟 。

### 4.4 TransD
TransD（Translating Embeddings for Modeling Multi-relational Data）模型是一种用于知识图谱嵌入的深度学习模型。它是由Zhen Wang等人在2014年提出的，旨在解决TransE模型中的一些限制。

TransD模型的主要思想是将实体和关系分别映射到两个不同的向量空间中。在TransD中，每个关系都有一个关系向量，每个实体都有一个实体向量。模型通过定义一种映射函数，将实体向量从一个空间转换到关系所在的空间。这种映射函数的作用是将实体和关系在不同的空间中进行分离，使得实体和关系之间的语义关联更加清晰。

TransD模型的损失函数也与TransE模型不同。TransD使用了margin-based的排名损失函数，目标是使得正确的三元组得分高于负样本的得分。这样，模型可以更好地学习实体和关系之间的语义关联。

TransD模型通过将实体和关系映射到不同的向量空间，并使用margin-based的损失函数，来提高知识图谱中实体和关系嵌入的质量和表达能力。这使得模型更加灵活和强大，能够更好地处理多关系的知识图谱数据。

TransD 模型在链接预测和三元组分类方面的性能相比 TransE、TransR、TransH 模型也更加优越，能够更好地捕获用于电影推荐的知识图谱之间的非线性关系。

#### 4.4.1 PTransD
2022年广东工业大学张欣提出 PTransD 模型，使用聚类算法和概率分布相似的原理来分别克服 TransD 的模型参数多和实体两种表示之间无联系的缺陷，并将得分函数的距离模型和概率分布相似模型 集成为一个模型，从而增强模型的表示能力。

## 5 相关 - 图表示学习
知识图谱是图的一种，了解常用图表示学习有助于优化知识图谱的表示。多跳实体之间往往也有相关性，为了聚合多跳实体隐含的语义到实体上，一般采用传播和聚合的方法。

图嵌入 (Graph Embedding)，也叫网络表示学习 (Network Representation Learn- ing), 主要指为图中的每个节点学习到一个低维稠密向量表示。学到的低维表征既编码了节点的属性信息，又可以表示节点在图中的结构信息，这个低维向量可以用于例如推荐等下游任务，抑或是用于诸如节点分类，链路预测或是聚类等图形分析任务。

基于 GNN 的嵌入模型的输入，通常是节点的属性向量，其编码函数是一个多层图神经网络，它不断地 聚合每个中心节点周围的邻居节点的特征，作为当前 中心节点的特征表示的更新。

由于有些实体的领居节点数量过式，导致算法效率低下，故常对数据进行采样处理。采样策略为，对每个实体，设定一个采样值K，当项目的邻域节点个数小于预先采样值K时，将所有领域实体考虑在内，否则根据实体与领域的皮尔逊相关系数，选择前K个领域节点。从而得到采样后的知识图谱。

之后对采样后的知识图谱进行构图，可以采用 DGL（Deep Graph Library）技术。DGL可以很好地对图神经网络进行简化计算，训练速度快，处理数据量大，不仅支持同构图，而且全面支持异构图。

图嵌入方法主要分为三类，分别为基于因子分解的方法，基于随机游走的方法以及基于神经网络的方法。其中比较常用的方法有 DeepWalk，Line， Node2Vec 以及 TransE 等。

### 5.1 DeepWalk
Deepwalk 是由 Perozzi B 等人于 2014 年提出的一种基于随机游走的图嵌入模型。作者在 YouTube 社交网络网络和 Wikipedia 文本数据上进行实验，发现 在随机游走过程中网络节点被访问的频率和文本中单词出现的频率具有相似的分布，由此作者认为在词嵌入中表现优异的 Word2vec 模型也可以迁移到图嵌入的领域中，并且发挥不错的效果。

DeepWalk 模型主要包括随机游走和 Skip-gram 两个部分。该模型将网络结 构信息作为输入，根据网络节点间是否存在连接边，在网络结构中进行随机游 走生成多个固定长度的节点序列，从而将网络结构信息转化为自然语言处理中 的单词序列。然后将这些序列作为 Skip-gram 模型的输入，最终学习到网络中每 个节点的低维向量表示。

### 5.2 Line
Line 算法利用广度优先遍历的思想构造目标节点的邻 域，该算法不仅考虑直接邻居，还认为拥有较多共同二阶邻居的节点也具有较大的相似度。

### 5.3 Node2vec
斯坦福大学于 2016 年提出的 Node2vec 模型对 DeepWalk 模型的随机游走部分进行了改进和创新。该模型认为，图中节点间的相似性不仅与同质性有关，还与同构性有关。具体而言，同质性指距离相近的节点的低维向量表示应尽量相似, 如节点 u 与其相邻节点 s1, s2, s3, s4;同构性指结构相似的节点的低维向量表示也应该相似，如节点 u 和节点 s6。因此，Node2vec 指出应该使用灵活的算法，以使模型最终学到的节点表示在同质性和同构性上都有体现。

具体而言，Node2vec 提出随机游走时的两种方式:深度优先搜索 (DFS) 和 广度优先搜索 (BFS)。DFS 学习到的节点表示更能保存节点的同构性，因为 DFS 更倾向于多次跳跃游走到远处节点，使得生成的序列包含更多网络的整体结构信息;而 BFS 由于更倾向于游走到和当前节点直接相 连的节点，因此就会有更多同质性信息包含在生成的序列样本中。

### 5.4 基于空间方法的图神经网络模型
基于空间方法的图神经网络的主要思想是从节点出发，利用图上的信息传递，用中心节点的邻居节点来更新中心节点的状态。基于聚合函数，使每一个节点的表示是其周围节点与自身信息的叠加。消息传播可分为两个步骤:首先，通过聚合函数得到节点的邻居节点表达。之后，通过使用更新函数将节点与邻居节点的表示进行聚合，得到节点更新后的表示。

#### 5.4.1 GAT
该模型在传统的图神经网络中加上了注意力机制，目的是能够辨别不同邻居节点的重要性。

##### 5.4.1.1 GraphSage
图注意力网络模型中节点之间的注意力权重需要图中所有节点的向量表示，这就导致了其在数据规模较大时具有局限性。GraphSAGE（Graph Sample and Aggregated）是一种用于图神经网络的节点表示学习方法，旨在学习图中节点的低维嵌入表示。它通过利用局部邻域信息和聚合操作，将节点的邻居特征进行采样和聚合，从而捕捉节点在图结构中的上下文信息。

GraphSAGE 的主要思想是，通过采样节点的邻居子图，然后使用聚合函数将邻居节点的特征进行汇总，从而生成节点的表示向量。这样的设计使得 GraphSAGE 能够处理大规模图数据，并能够对节点进行有效的表示学习。

GraphSAGE 的工作流程如下：
1. 邻居采样：对于每个节点，从其邻居节点中采样出固定数量的邻居子图。
2. 特征聚合：对于每个邻居子图，使用聚合函数（如平均池化或最大池化）将邻居节点的特征进行聚合，得到一个邻居子图的特征向量。
3. 节点表示学习：将节点的特征向量与节点自身的特征进行合并，并通过神经网络模型进行节点表示学习，生成节点的低维嵌入表示。

#### 5.4.2 图卷积网络 GCN
图卷积网络是一种处理节点图数据的半监督深度学习模型。它主要是将普通的卷积神经网络推广到节点图数据，从而对节点进行特征提取。GCN 同时具备层级结构，非线性变化和端到端训练三种特性，所以较传统的标签传播算法有更好的性能。
图卷积通过随机初始化得到的网络来提取节点的特征表示，网络中的每一层都按拓扑结构进行处理。首先，每一个节点将自身的特征信息经过变换后发送给它的邻居节点，然后各节点将这些传递的特征信息进行聚集，最后对节点信息进行非线性变换。

##### 5.4.2.1 PinSage
PinSage（PinSage: A Graph Convolutional Neural Network for Web-Scale Recommender Systems）是一种用于大规模推荐系统的图卷积神经网络模型。它旨在解决在大规模网络中进行个性化推荐的问题，特别是在面对大量用户和物品节点以及复杂的关系网络时。

PinSage基于图卷积神经网络（Graph Convolutional Neural Network，GCN）的思想，利用图结构中的节点和边关系来学习节点的特征表示。它采用了一种层级采样的方法，通过对图进行多层次的采样，从而构建了一个包含多个邻居节点的子图。这样做的目的是捕捉节点的上下文信息和邻居节点的特征。

PinSage模型的核心是采用了两个重要的组件：图采样层和图卷积层。图采样层用于对原始图进行层级采样，构建包含邻居节点的子图。图卷积层通过图卷积操作对子图中的节点进行特征更新和信息传播。模型通过多次迭代，逐步聚合和更新节点特征，得到最终的节点表示。

PinSage模型的优点在于，它能够处理大规模的图数据并进行高效的推荐计算。它通过利用图结构和多层次采样的方法，有效地捕捉节点之间的关系和上下文信息，从而提高推荐的准确性和个性化程度。

总之，PinSage模型通过图卷积神经网络和层级采样的策略，为大规模推荐系统提供了一种有效的建模方法，能够处理复杂的关系网络，并进行准确的个性化推荐。

##### 5.4.2.2 DKN
DKN（Deep Knowledge-Aware Network）是一种用于推荐系统的深度学习模型，旨在利用用户行为数据和知识图谱的信息来提升推荐的效果。DKN的原理是结合了深度神经网络和知识图谱的表示学习，通过同时学习用户行为和知识图谱的特征来进行个性化推荐。

DKN模型的核心思想是将用户的行为数据和知识图谱进行融合，以获取更丰富的特征表示。它通过使用图卷积神经网络（Graph Convolutional Neural Network，GCN）来学习知识图谱中实体和关系的表示，同时结合用户行为数据中的点击、购买等行为，形成用户行为的特征表示。

具体来说，DKN模型首先利用知识图谱中的实体和关系构建图结构，并通过图卷积操作学习实体和关系的表示向量。然后，它将用户的行为数据映射为用户行为特征，例如点击序列的embedding表示。接下来，DKN通过结合知识图谱的表示和用户行为的特征，生成综合的用户兴趣表示。最后，模型使用这些用户兴趣表示来进行推荐，预测用户对未观看内容的兴趣度。

DKN模型的优点在于，它能够利用知识图谱中的丰富信息来丰富推荐过程，提高推荐的准确性和个性化程度。它能够学习到用户的兴趣和知识之间的关联，从而更好地理解用户的需求和兴趣。

综上所述，DKN模型通过融合用户行为数据和知识图谱的信息，利用深度学习模型进行特征学习和推荐，为个性化推荐系统提供了一种有效的方法。

##### 5.4.2.3 关系图卷积网络
GCN 在处理图结构数据时，忽略了不同类型的边对节点的影响。为了解决这个问题，关系图卷积网络（Relational Graph Convolutional Network, RGCN）被提出。

关系图卷积网络（Relational Graph Convolutional Network，R-GCN）是一种用于图数据的卷积神经网络模型，用于学习节点之间的关系和图结构中的特征表示。它考虑了节点之间的关系，并利用关系矩阵来定义图卷积操作，从而实现节点特征的更新和信息传播。R-GCN的原理是通过对节点和边同时进行卷积操作，从而捕捉节点在不同关系下的特征表达。

#### 5.4.3 图自编码器
图自编码器（Graph Autoencoder）是一种用于图数据的无监督学习模型，旨在学习图数据的紧凑、低维表示。它基于自编码器的思想，通过将图结构编码为低维向量表示，然后再将该向量表示解码为重构的图结构。

### 5.5 基于谱方法的图神经网络模型
基于谱的图卷积网络，主要研究图的邻接矩阵，借助图的拉普拉斯矩阵的特征向量与节点特征向量，来研究图的性质。

### 5.6 图生成对抗网络 GraphGAN
GraphGAN 是一种在 GAN 的基础上，将生成式模型与判别式模型结合的关于 节点表示的模型。其中生成器 G 的目的是尽量生成近似真实的节点连接分布 𝑝(𝑣|𝑣𝑐)，同时判别器 D 的目的是尽量判别节点𝑣和𝑣𝑐之间存在边的概率。

## 6 相关知识 - 词嵌入
如何对文本进行特征表示一直是自然语言处理领域的热点研究问题。

传统的独热向量式的表示方法往往会因词汇量的巨大而导致特征向量维度过高、稀疏性较大的问题。且此类方法不能刻画出词汇间的深层语言信息，忽略了词汇间的关联，不利于后续的建模。

词嵌入 (Word Embedding) 是目前常用的一种单词特征表示算法，通过使用神经网络方法对词库进行建模，将每个单词表示为一个低维稠密的向量，并在一定程度上保留了词汇之间的语义信息。当前应用最广泛的一种词嵌入技术是由 Mikolov 在 2013 年提出的 Word2vec。根据训练方式的不同，Word2vec 可分为 Skip-gram 和 CBOW 两种方法。主要区别在于前者是使用当前词来预测上下文词，而后者使用上下文词预测当前词。同时 Word2vec 还有两种优化方法 来提高词向量的训练效率，分别是层次 Softmax 和负采样技术。

### 6.1 Skip-gram 模型
Skip-gram 模型的主要思想是采用中心词来预测上下文词。通过一个固定 长度的滑动窗口扫描语词库中的每个文本，将窗口中心的词作为中心词，窗口 中的其他词作为上下文词，以此得到训练样本。同时构造一个只有一个隐藏层 的神经网络进行训练。由于输入层与隐藏层之间的权重可以与每个单词一一对 应，所以可以将训练结束后的该部分权重将作为词向量.

### 6.2 负采样技术
原始 Skip-gram 模型每更新一次权重就要遍历整个词库，这对于大规模语料库的训练在计算效率上将会是一个很大的挑战，采用负采样技术则可以有效地解决这一问题，实现训练过程的加速。

负采样 (Negative Sampling) 的想法比较简单直接:针对每个训练样本，原始的训练方法都要更新整个输出层权重 W ′, 而负采样技术则选择只更新 W ′ 的一部分。在用滑动窗口扫描词库时，将窗口中心的词作为中心词，窗口中的其他词作为上下文词 (此时也可称为正样本)，并针对每个上下文词，在整个词库中随机抽取固定数量的词作为负样本。
